<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>Introduction to deep learning: Advanced layer types</title><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" type="text/css" href="assets/styles.css"><script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png"><link rel="manifest" href="site.webmanifest"><link rel="mask-icon" href="safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" content="#ffffff"></head><body>
    <header id="top" class="navbar navbar-expand-md navbar-light bg-white top-nav incubator"><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-6">
      <div class="large-logo">
        <img alt="Carpentries Incubator" src="assets/images/incubator-logo.svg"></div>
    </div>
    <div class="selector-container">
      
      
      <div class="dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/4-advanced-layer-types.html';">Instructor View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl navbar-light bg-white bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="assets/images/incubator-logo-sm.svg"></div>
    <div class="lesson-title-md">
      Introduction to deep learning
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            Introduction to deep learning
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"><li><a class="dropdown-item" href="reference.html">Reference</a></li>
          </ul></li>
      </ul></div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a class="btn btn-primary" href="aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Introduction to deep learning
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 63%" class="percentage">
    63%
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: 63%" aria-valuenow="63" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/4-advanced-layer-types.html">Instructor View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->
      
            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="1-introduction.html">1. Introduction</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="2-keras.html">2. Classification by a neural network using Keras</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="3-monitor-the-model.html">3. Monitor the training process</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapsecurrent" aria-expanded="true" aria-controls="flush-collapsecurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        4. Advanced layer types
        </span>
      </button>
    </div><!--/div.accordion-header-->
        
    <div id="flush-collapsecurrent" class="accordion-collapse collapse show" aria-labelledby="flush-headingcurrent" data-bs-parent="#accordionFlushcurrent">
      <div class="accordion-body">
        <ul><li><a href="#different-types-of-layers">Different types of layers</a></li>
<li><a href="#formulate-outline-the-problem-image-classification">1. Formulate / Outline the problem: Image classification</a></li>
<li><a href="#dollar-street-10-dataset">Dollar Street 10 dataset</a></li>
<li><a href="#identify-inputs-and-outputs">2. Identify inputs and outputs</a></li>
<li><a href="#prepare-data">3. Prepare data</a></li>
<li><a href="#choose-a-pretrained-model-or-start-building-architecture-from-scratch">4. Choose a pretrained model or start building architecture from
scratch</a></li>
<li><a href="#convolutional-layers">Convolutional layers</a></li>
<li><a href="#pooling-layers">Pooling layers</a></li>
<li><a href="#choose-a-loss-function-and-optimizer">5. Choose a loss function and optimizer</a></li>
<li><a href="#train-the-model">6. Train the model</a></li>
<li><a href="#perform-a-predictionclassification">7. Perform a Prediction/Classification</a></li>
<li><a href="#measure-performance">8. Measure performance</a></li>
<li><a href="#refine-the-model">9. Refine the model</a></li>
<li><a href="#dropout">Dropout</a></li>
<li><a href="#hyperparameter-tuning">Hyperparameter tuning</a></li>
<li><a href="#share-model">10. Share model</a></li>
<li><a href="#conclusion-and-next-steps">Conclusion and next steps</a></li>
        </ul></div><!--/div.accordion-body-->
    </div><!--/div.accordion-collapse-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="5-transfer-learning.html">5. Transfer learning</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="6-outlook.html">6. Outlook</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>
                      <li><a href="reference.html">Reference</a></li>
                    </ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width resources"><a href="aio.html">See all in one page</a>
            

            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">
            
            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="3-monitor-the-model.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="5-transfer-learning.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="3-monitor-the-model.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Monitor the training
        </a>
        <a class="chapter-link float-end" href="5-transfer-learning.html" rel="next">
          Next: Transfer learning... 
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>Advanced layer types</h1>
        <p>Last updated on 2024-06-11 |
        
        <a href="https://github.com/carpentries-incubator/deep-learning-intro/edit/main/episodes/4-advanced-layer-types.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
        
        
        
        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul><li>Why do we need different types of layers?</li>
<li>What are good network designs for image data?</li>
<li>What is a convolutional layer?</li>
<li>How can we use different types of layers to prevent
overfitting?</li>
<li>What is hyperparameter tuning?</li>
</ul></div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul><li>Understand why convolutional and pooling layers are useful for image
data</li>
<li>Implement a convolutional neural network on an image dataset</li>
<li>Use a drop-out layer to prevent overfitting</li>
<li>Be able to tune the hyperparameters of a Keras model</li>
</ul></div>
</div>
</div>
</div>
</div>
<section id="different-types-of-layers"><h2 class="section-heading">Different types of layers<a class="anchor" aria-label="anchor" href="#different-types-of-layers"></a>
</h2>
<hr class="half-width"><p>Networks are like onions: a typical neural network consists of many
layers. In fact, the word <em>deep</em> in <em>deep learning</em> refers
to the many layers that make the network deep.</p>
<p>So far, we have seen one type of layer, namely the <strong>fully
connected</strong>, or <strong>dense</strong> layer. This layer is
called fully connected, because all input neurons are taken into account
by each output neuron. The number of parameters that need to be learned
by the network, is thus in the order of magnitude of the number of input
neurons times the number of hidden neurons.</p>
<p>However, there are many different types of layers that perform
different calculations and take different inputs. In this episode we
will take a look at <strong>convolutional layers</strong> and
<strong>dropout layers</strong>, which are useful in the context of
image data, but also in many other types of (structured) data.</p>
</section><section id="formulate-outline-the-problem-image-classification"><h2 class="section-heading">1. Formulate / Outline the problem: Image classification<a class="anchor" aria-label="anchor" href="#formulate-outline-the-problem-image-classification"></a>
</h2>
<hr class="half-width"><p>The <a href="https://www.kaggle.com/datasets/mlcommons/the-dollar-street-dataset" class="external-link">MLCommons
Dollar Street Dataset</a> is a collection of images of everyday
household items from homes around the world that visually captures
socioeconomic diversity of traditionally underrepresented populations.
We use <a href="https://zenodo.org/records/10970014" class="external-link">a subset of the
original dataset</a> that can be used for multiclass classification with
10 categories. Let’s load the data:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> pathlib</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>DATA_FOLDER <span class="op">=</span> pathlib.Path(<span class="st">'data/dataset_dollarstreet/'</span>) <span class="co"># change to location where you stored the data</span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>train_images <span class="op">=</span> np.load(DATA_FOLDER <span class="op">/</span> <span class="st">'train_images.npy'</span>)</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>val_images <span class="op">=</span> np.load(DATA_FOLDER <span class="op">/</span> <span class="st">'test_images.npy'</span>)</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>train_labels <span class="op">=</span> np.load(DATA_FOLDER <span class="op">/</span> <span class="st">'train_labels.npy'</span>)</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>val_labels <span class="op">=</span> np.load(DATA_FOLDER <span class="op">/</span> <span class="st">'test_labels.npy'</span>)</span></code></pre>
</div>
<div id="a-note-about-data-provenance" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="a-note-about-data-provenance" class="callout-inner">
<h3 class="callout-title">A Note About Data Provenance<a class="anchor" aria-label="anchor" href="#a-note-about-data-provenance"></a>
</h3>
<div class="callout-content">
<p>In an earlier version, this part of the lesson used a different
example dataset. During <a href="https://github.com/carpentries-lab/reviews/issues/25#issuecomment-1953271802" class="external-link">peer
review</a>, the decision was made to replace that dataset due to the way
it had been compiled using images “scraped” from the internet without
permission from or credit to the original creators of those images.
Unfortunately, uncredited use of images is a common problem among
datasets used to benchmark models for image classification. The Dollar
Street Dataset was chosen for use in the lesson as it contains only
images <a href="https://www.gapminder.org/dollar-street/about?" class="external-link">created
by the Gapminder project</a> expressly for the purposes of use in the
dataset. The original Dollar Street Dataset is very large – more than
100 GB – with the potential to grow even bigger, so we created a subset
for use in this lesson.</p>
</div>
</div>
</div>
</section><section id="dollar-street-10-dataset"><h2 class="section-heading">Dollar Street 10 dataset<a class="anchor" aria-label="anchor" href="#dollar-street-10-dataset"></a>
</h2>
<hr class="half-width"><p>The dollar street 10 dataset consists of images of 10 different
classes, this is the mapping of the categories:</p>
<table class="table"><thead><tr class="header"><th>Category</th>
<th>label</th>
</tr></thead><tbody><tr class="odd"><td>day bed</td>
<td>0</td>
</tr><tr class="even"><td>dishrag</td>
<td>1</td>
</tr><tr class="odd"><td>plate</td>
<td>2</td>
</tr><tr class="even"><td>running shoe</td>
<td>3</td>
</tr><tr class="odd"><td>soap dispenser</td>
<td>4</td>
</tr><tr class="even"><td>street sign</td>
<td>5</td>
</tr><tr class="odd"><td>table lamp</td>
<td>6</td>
</tr><tr class="even"><td>tile roof</td>
<td>7</td>
</tr><tr class="odd"><td>toilet seat</td>
<td>8</td>
</tr><tr class="even"><td>washing machine</td>
<td>9</td>
</tr></tbody></table><figure><img src="fig/04_dollar_street_10.png" alt="A 5 by 5 grid of 25 sample images from the dollar street 10 data-set. Each image is labelled with a category, for example: 'street sign' or 'soap dispenser'." class="figure mx-auto d-block"><div class="figcaption">Sample images from the dollar street 10
data-set. Each image is labelled with a category, for example: ‘street
sign’ or ‘soap dispenser’</div>
</figure></section><section id="identify-inputs-and-outputs"><h2 class="section-heading">2. Identify inputs and outputs<a class="anchor" aria-label="anchor" href="#identify-inputs-and-outputs"></a>
</h2>
<hr class="half-width"><div class="section level3">
<h3 id="explore-the-data">Explore the data<a class="anchor" aria-label="anchor" href="#explore-the-data"></a></h3>
<p>Let’s do a quick exploration of the dimensions of the data:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>train_images.shape</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(878, 64, 64, 3)</code></pre>
</div>
<p>The first value, <code>878</code>, is the number of training images
in the dataset. The remainder of the shape, namely
<code>(64, 64, 3)</code>, denotes the dimension of one image. The last
value 3 is typical for color images, and stands for the three color
channels <strong>R</strong>ed, <strong>G</strong>reen,
<strong>B</strong>lue.</p>
<div id="number-of-features-dollar-street-10" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="number-of-features-dollar-street-10" class="callout-inner">
<h3 class="callout-title">Number of features dollar-street-10<a class="anchor" aria-label="anchor" href="#number-of-features-dollar-street-10"></a>
</h3>
<div class="callout-content">
<p>How many features does one image in the dollar-street-10 dataset
have?</p>
<ul><li>A. 64</li>
<li>B. 4096</li>
<li>C. 12288</li>
<li>D. 878</li>
</ul></div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>The correct solution is C: 12288</p>
<p>There are 4096 pixels in one image (64 * 64), each pixel has 3
channels (RGB). So 4096 * 3 = 12288.</p>
</div>
</div>
</div>
</div>
<p>We can find out the range of values of our input data as follows:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>train_images.<span class="bu">min</span>(), train_images.<span class="bu">max</span>()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(0, 255)</code></pre>
</div>
<p>So the values of the three channels range between <code>0</code> and
<code>255</code>. Lastly, we inspect the dimension of the labels:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>train_labels.shape</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(878, 1)</code></pre>
</div>
<p>So we have, for each image, a single value denoting the label. To
find out what the possible values of these labels are:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>train_labels.<span class="bu">min</span>(), train_labels.<span class="bu">max</span>()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(0, 9)</code></pre>
</div>
<p>The values of the labels range between <code>0</code> and
<code>9</code>, denoting 10 different classes.</p>
</div>
</section><section id="prepare-data"><h2 class="section-heading">3. Prepare data<a class="anchor" aria-label="anchor" href="#prepare-data"></a>
</h2>
<hr class="half-width"><p>The training set consists of 878 images of <code>64x64</code> pixels
and 3 channels (RGB values). The RGB values are between <code>0</code>
and <code>255</code>. For input of neural networks, it is better to have
small input values. So we normalize our data between <code>0</code> and
<code>1</code>:</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>train_images <span class="op">=</span> train_images <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>val_images <span class="op">=</span> val_images <span class="op">/</span> <span class="fl">255.0</span></span></code></pre>
</div>
</section><section id="choose-a-pretrained-model-or-start-building-architecture-from-scratch"><h2 class="section-heading">4. Choose a pretrained model or start building architecture from
scratch<a class="anchor" aria-label="anchor" href="#choose-a-pretrained-model-or-start-building-architecture-from-scratch"></a>
</h2>
<hr class="half-width"></section><section id="convolutional-layers"><h2 class="section-heading">Convolutional layers<a class="anchor" aria-label="anchor" href="#convolutional-layers"></a>
</h2>
<hr class="half-width"><p>In the previous episodes, we used ‘fully connected layers’ , that
connected all input values of a layer to all outputs of a layer. This
results in many connections, and thus many weights to be learned, in the
network. Note that our input dimension is now quite high (even with
small pictures of <code>64x64</code> pixels): we have 12288
features.</p>
<div id="parameters-exercise-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="parameters-exercise-1" class="callout-inner">
<h3 class="callout-title">Number of parameters<a class="anchor" aria-label="anchor" href="#parameters-exercise-1"></a>
</h3>
<div class="callout-content">
<p>Suppose we create a single Dense (fully connected) layer with 100
hidden units that connect to the input pixels, how many parameters does
this layer have?</p>
<ul><li>A. 1228800</li>
<li>B. 1228900</li>
<li>C. 100</li>
<li>D. 12288</li>
</ul></div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p>The correct answer is B: Each entry of the input dimensions, i.e. the
<code>shape</code> of one single data point, is connected with 100
neurons of our hidden layer, and each of these neurons has a bias term
associated to it. So we have <code>1228900</code> parameters to
learn.</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>width, height <span class="op">=</span> (<span class="dv">64</span>, <span class="dv">64</span>)</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>n_hidden_neurons <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>n_bias <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>n_input_items <span class="op">=</span> width <span class="op">*</span> height <span class="op">*</span> <span class="dv">3</span></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>n_parameters <span class="op">=</span> (n_input_items <span class="op">*</span> n_hidden_neurons) <span class="op">+</span> n_bias</span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>n_parameters</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="fl">1228900</span></span></code></pre>
</div>
<p>We can also check this by building the layer in Keras:</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(n_input_items,))</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>outputs <span class="op">=</span> keras.layers.Dense(<span class="dv">100</span>)(inputs)</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>model <span class="op">=</span> keras.models.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>outputs)</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>model.summary()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 input_1 (InputLayer)        [(None, 12288)]           0

 dense (Dense)               (None, 100)               1228900

=================================================================
Total params: 1228900 (4.69 MB)
Trainable params: 1228900 (4.69 MB)
Non-trainable params: 0 (0.00 Byte)</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>We can decrease the number of units in our hidden layer, but this
also decreases the number of patterns our network can remember.
Moreover, if we increase the image size, the number of weights will
‘explode’, even though the task of recognizing large images is not
necessarily more difficult than the task of recognizing small
images.</p>
<p>The solution is that we make the network learn in a ‘smart’ way. The
features that we learn should be similar both for small and large
images, and similar features (e.g. edges, corners) can appear anywhere
in the image (in mathematical terms: <em>translation invariant</em>). We
do this by making use of a concept from image processing that predates
deep learning.</p>
<p>A <strong>convolution matrix</strong>, or <strong>kernel</strong>, is
a matrix transformation that we ‘slide’ over the image to calculate
features at each position of the image. For each pixel, we calculate the
matrix product between the kernel and the pixel with its surroundings. A
kernel is typically small, between 3x3 and 7x7 pixels. We can for
example think of the 3x3 kernel:</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[[-1, -1, -1],
 [0, 0, 0]
 [1, 1, 1]]</code></pre>
</div>
<p>This kernel will give a high value to a pixel if it is on a
horizontal border between dark and light areas. Note that for RGB
images, the kernel should also have a depth of 3.</p>
<p>In the following image, we see the effect of such a kernel on the
values of a single-channel image. The red cell in the output matrix is
the result of multiplying and summing the values of the red square in
the input, and the kernel. Applying this kernel to a real image shows
that it indeed detects horizontal edges.</p>
<figure><img src="fig/04_conv_matrix.png" style="width:90%" alt="Example of a convolution matrix calculation" class="figure mx-auto d-block"></figure><figure><img src="fig/04_conv_image.png" style="width:100%" alt="Convolution example on an image of a cat to extract features" class="figure mx-auto d-block"></figure><p>In our <strong>convolutional layer</strong> our hidden units are a
number of convolutional matrices (or kernels), where the values of the
matrices are the weights that we learn in the training process. The
output of a convolutional layer is an ‘image’ for each of the kernels,
that gives the output of the kernel applied to each pixel.</p>
<div id="playing-with-convolutions" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="playing-with-convolutions" class="callout-inner">
<h3 class="callout-title">Playing with convolutions<a class="anchor" aria-label="anchor" href="#playing-with-convolutions"></a>
</h3>
<div class="callout-content">
<p>Convolutions applied to images can be hard to grasp at first.
Fortunately there are resources out there that enable users to
interactively play around with images and convolutions:</p>
<ul><li>
<a href="https://setosa.io/ev/image-kernels/" class="external-link">Image kernels
explained</a> shows how different convolutions can achieve certain
effects on an image, like sharpening and blurring.</li>
<li>
<a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks#" class="external-link">The
convolutional neural network cheat sheet</a> shows animated examples of
the different components of convolutional neural nets</li>
</ul></div>
</div>
</div>
<div id="border-pixels" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="border-pixels" class="callout-inner">
<h3 class="callout-title">Border pixels<a class="anchor" aria-label="anchor" href="#border-pixels"></a>
</h3>
<div class="callout-content">
<p>What, do you think, happens to the border pixels when applying a
convolution?</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<p>There are different ways of dealing with border pixels. You can
ignore them, which means that your output image is slightly smaller then
your input. It is also possible to ‘pad’ the borders, e.g. with the same
value or with zeros, so that the convolution can also be applied to the
border pixels. In that case, the output image will have the same size as
the input image.</p>
<p><a href="https://datacarpentry.org/image-processing/06-blurring.html#callout4" class="external-link">This
callout in the Data Carpentry: Image Processing with Python
curriculum</a> provides more detail about convolution at the boundaries
of an image, in the context of applying a <em>Gaussian blur</em>.</p>
</div>
</div>
</div>
</div>
<div id="number-of-model-parameters" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="number-of-model-parameters" class="callout-inner">
<h3 class="callout-title">Number of model parameters<a class="anchor" aria-label="anchor" href="#number-of-model-parameters"></a>
</h3>
<div class="callout-content">
<p>Suppose we apply a convolutional layer with 100 kernels of size 3 * 3
* 3 (the last dimension applies to the rgb channels) to our images of 32
* 32 * 3 pixels. How many parameters do we have? Assume, for simplicity,
that the kernels do not use bias terms. Compare this to the answer of
the earlier exercise, <a href="#parameters-exercise-1">“Number of
Parameters”</a>.</p>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4">Show me the solution</h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" aria-labelledby="headingSolution4" data-bs-parent="#accordionSolution4">
<div class="accordion-body">
<p>We have 100 matrices with 3 * 3 * 3 = 27 values each so that gives 27
* 100 = 2700 weights. This is a magnitude of 2000 less than the fully
connected layer with 100 units! Nevertheless, as we will see,
convolutional networks work very well for image data. This illustrates
the expressiveness of convolutional layers.</p>
</div>
</div>
</div>
</div>
<p>So let us look at a network with a few convolutional layers. We need
to finish with a Dense layer to connect the output cells of the
convolutional layer to the outputs for our classes.</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>train_images.shape[<span class="dv">1</span>:])</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(inputs)</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Flatten()(x)</span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>outputs <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>)(x)</span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a>model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>outputs, name<span class="op">=</span><span class="st">"dollar_street_model_small"</span>)</span>
<span id="cb16-8"><a href="#cb16-8" tabindex="-1"></a></span>
<span id="cb16-9"><a href="#cb16-9" tabindex="-1"></a>model.summary()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Model: "dollar_street_model_small"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 input_8 (InputLayer)        [(None, 64, 64, 3)]       0

 conv2d_10 (Conv2D)          (None, 62, 62, 50)        1400

 conv2d_11 (Conv2D)          (None, 60, 60, 50)        22550

 flatten_6 (Flatten)         (None, 180000)            0

 dense_14 (Dense)            (None, 10)                1800010

=================================================================
Total params: 1823960 (6.96 MB)
Trainable params: 1823960 (6.96 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</code></pre>
</div>
<div id="convolutional-neural-network" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="convolutional-neural-network" class="callout-inner">
<h3 class="callout-title">Convolutional Neural Network<a class="anchor" aria-label="anchor" href="#convolutional-neural-network"></a>
</h3>
<div class="callout-content">
<p>Inspect the network above:</p>
<ul><li>What do you think is the function of the <code>Flatten</code>
layer?</li>
<li>Which layer has the most parameters? Do you find this
intuitive?</li>
<li>(optional) This dataset is similar to the often used CIFAR-10
dataset. We can get inspiration for neural network architectures that
could work on our dataset here: <a href="https://paperswithcode.com/sota/image-classification-on-cifar-10" class="external-link uri">https://paperswithcode.com/sota/image-classification-on-cifar-10</a>
. Pick a model and try to understand how it works.</li>
</ul></div>
</div>
</div>
<div id="accordionSolution5" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution5" aria-expanded="false" aria-controls="collapseSolution5">
  <h4 class="accordion-header" id="headingSolution5">Show me the solution</h4>
</button>
<div id="collapseSolution5" class="accordion-collapse collapse" aria-labelledby="headingSolution5" data-bs-parent="#accordionSolution5">
<div class="accordion-body">
<ul><li>The Flatten layer converts the 60x60x50 output of the convolutional
layer into a single one-dimensional vector, that can be used as input
for a dense layer.</li>
<li>The last dense layer has the most parameters. This layer connects
every single output ‘pixel’ from the convolutional layer to the 10
output classes. That results in a large number of connections, so a
large number of parameters. This undermines a bit the expressiveness of
the convolutional layers, that have much fewer parameters.</li>
</ul></div>
</div>
</div>
</div>
<div id="search-for-existing-architectures-or-pretrained-models" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="search-for-existing-architectures-or-pretrained-models" class="callout-inner">
<h3 class="callout-title">Search for existing architectures or
pretrained models<a class="anchor" aria-label="anchor" href="#search-for-existing-architectures-or-pretrained-models"></a>
</h3>
<div class="callout-content">
<p>So far in this course we have built neural networks from scratch,
because we want you to fully understand the basics of Keras. In the real
world however, you would first search for existing solutions to your
problem.</p>
<p>You could for example search for ‘large CNN image classification
Keras implementation’, and see if you can find any Keras implementations
of more advanced architectures that you could reuse. A lot of the
best-performing architectures for image classification are convolutional
neural networks or at least have some elements in common. Therefore, we
will introduce convolutional neural networks here, and the best way to
teach you is by developing a neural network from scratch!</p>
</div>
</div>
</div>

</section><section id="pooling-layers"><h2 class="section-heading">Pooling layers<a class="anchor" aria-label="anchor" href="#pooling-layers"></a>
</h2>
<hr class="half-width"><p>Often in convolutional neural networks, the convolutional layers are
intertwined with <strong>Pooling layers</strong>. As opposed to the
convolutional layer, the pooling layer actually alters the dimensions of
the image and reduces it by a scaling factor. It is basically decreasing
the resolution of your picture. The rationale behind this is that higher
layers of the network should focus on higher-level features of the
image. By introducing a pooling layer, the subsequent convolutional
layer has a broader ‘view’ on the original image.</p>
<p>Let’s put it into practice. We compose a Convolutional network with
two convolutional layers and two pooling layers.</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="kw">def</span> create_nn():</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a>    inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>train_images.shape[<span class="dv">1</span>:])</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(inputs)</span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x) <span class="co"># a new maxpooling layer</span></span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x) <span class="co"># a new maxpooling layer (same as maxpool)</span></span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Flatten()(x)</span>
<span id="cb18-8"><a href="#cb18-8" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Dense(<span class="dv">50</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x) <span class="co"># a new Dense layer</span></span>
<span id="cb18-9"><a href="#cb18-9" tabindex="-1"></a>    outputs <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>)(x)</span>
<span id="cb18-10"><a href="#cb18-10" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" tabindex="-1"></a>    model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>outputs, name<span class="op">=</span><span class="st">"dollar_street_model"</span>)</span>
<span id="cb18-12"><a href="#cb18-12" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb18-13"><a href="#cb18-13" tabindex="-1"></a></span>
<span id="cb18-14"><a href="#cb18-14" tabindex="-1"></a>model <span class="op">=</span> create_nn()</span>
<span id="cb18-15"><a href="#cb18-15" tabindex="-1"></a>model.summary()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Model: "dollar_street_model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 input_3 (InputLayer)        [(None, 64, 64, 3)]       0

 conv2d_2 (Conv2D)           (None, 62, 62, 50)        1400

 max_pooling2d (MaxPooling2  (None, 31, 31, 50)        0
 D)

 conv2d_3 (Conv2D)           (None, 29, 29, 50)        22550

 max_pooling2d_1 (MaxPoolin  (None, 14, 14, 50)        0
 g2D)

 flatten_1 (Flatten)         (None, 9800)              0

 dense_2 (Dense)             (None, 50)                490050

 dense_3 (Dense)             (None, 10)                510

=================================================================
Total params: 514510 (1.96 MB)
Trainable params: 514510 (1.96 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________</code></pre>
</div>
</section><section id="choose-a-loss-function-and-optimizer"><h2 class="section-heading">5. Choose a loss function and optimizer<a class="anchor" aria-label="anchor" href="#choose-a-loss-function-and-optimizer"></a>
</h2>
<hr class="half-width"><p>We compile the model using the adam optimizer (other optimizers could
also be used here!). Similar to the penguin classification task, we will
use the crossentropy function to calculate the model’s loss. This loss
function is appropriate to use when the data has two or more label
classes.</p>
<p>Remember that our target class is represented by a single integer,
whereas the output of our network has 10 nodes, one for each class. So,
we should have actually one-hot encoded the targets and used a softmax
activation for the neurons in our output layer! Luckily, there is a
quick fix to calculate crossentropy loss for data that has its classes
represented by integers, the
<code>SparseCategoricalCrossentropy()</code> function. Adding the
argument <code>from_logits=True</code> accounts for the fact that the
output has a linear activation instead of softmax. This is what is often
done in practice, because it spares you from having to worry about
one-hot encoding.</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="kw">def</span> compile_model(model):</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a>    model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'adam'</span>,</span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a>                  loss<span class="op">=</span>keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a>                  metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a>compile_model(model)</span></code></pre>
</div>

</section><section id="train-the-model"><h2 class="section-heading">6. Train the model<a class="anchor" aria-label="anchor" href="#train-the-model"></a>
</h2>
<hr class="half-width"><p>We then train the model for 10 epochs:</p>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a>history <span class="op">=</span> model.fit(train_images, train_labels, epochs<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a>                    validation_data<span class="op">=</span>(val_images, val_labels))</span></code></pre>
</div>
</section><section id="perform-a-predictionclassification"><h2 class="section-heading">7. Perform a Prediction/Classification<a class="anchor" aria-label="anchor" href="#perform-a-predictionclassification"></a>
</h2>
<hr class="half-width"><p>Here we skip performing a prediction, and continue to measuring the
performance. In practice, you will only do this step once in a while
when you actually need to have the individual predictions, often you
know enough based on the evaluation metric scores. Of course, behind the
scenes whenever you measure performance you have to make predictions and
compare them to the ground truth.</p>
</section><section id="measure-performance"><h2 class="section-heading">8. Measure performance<a class="anchor" aria-label="anchor" href="#measure-performance"></a>
</h2>
<hr class="half-width"><p>We can plot the training process using the history:</p>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb22-4"><a href="#cb22-4" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" tabindex="-1"></a><span class="kw">def</span> plot_history(history, metrics):</span>
<span id="cb22-6"><a href="#cb22-6" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb22-7"><a href="#cb22-7" tabindex="-1"></a><span class="co">    Plot the training history</span></span>
<span id="cb22-8"><a href="#cb22-8" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb22-10"><a href="#cb22-10" tabindex="-1"></a><span class="co">        history (keras History object that is returned by model.fit())</span></span>
<span id="cb22-11"><a href="#cb22-11" tabindex="-1"></a><span class="co">        metrics(str, list): Metric or a list of metrics to plot</span></span>
<span id="cb22-12"><a href="#cb22-12" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb22-13"><a href="#cb22-13" tabindex="-1"></a>    history_df <span class="op">=</span> pd.DataFrame.from_dict(history.history)</span>
<span id="cb22-14"><a href="#cb22-14" tabindex="-1"></a>    sns.lineplot(data<span class="op">=</span>history_df[metrics])</span>
<span id="cb22-15"><a href="#cb22-15" tabindex="-1"></a>    plt.xlabel(<span class="st">"epochs"</span>)</span>
<span id="cb22-16"><a href="#cb22-16" tabindex="-1"></a>    plt.ylabel(<span class="st">"metric"</span>)</span>
<span id="cb22-17"><a href="#cb22-17" tabindex="-1"></a>plot_history(history, [<span class="st">'accuracy'</span>, <span class="st">'val_accuracy'</span>])</span></code></pre>
</div>
<figure><img src="fig/04_training_history_1.png" alt="Plot of training accuracy and validation accuracy vs epochs for the trained model" class="figure mx-auto d-block"></figure><div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a>plot_history(history, [<span class="st">'loss'</span>, <span class="st">'val_loss'</span>])</span></code></pre>
</div>
<figure><img src="fig/04_training_history_loss_1.png" alt="Plot of training loss and validation loss vs epochs for the trained model" class="figure mx-auto d-block"></figure><p>It seems that the model is overfitting a lot, because the training
accuracy increases, while the validation accuracy stagnates. Meanwhile,
the training loss keeps decreasing while the validation loss actually
starts increasing after a few epochs.</p>

<div id="comparison-with-a-network-with-only-dense-layers-1" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="comparison-with-a-network-with-only-dense-layers-1" class="callout-inner">
<h3 class="callout-title">Comparison with a network with only dense
layers<a class="anchor" aria-label="anchor" href="#comparison-with-a-network-with-only-dense-layers-1"></a>
</h3>
<div class="callout-content">
<p>How does this simple CNN compare to a neural network with only dense
layers?</p>
<p>We can define a neural network with only dense layers:</p>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a><span class="kw">def</span> create_dense_model():</span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a>    inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>train_images.shape[<span class="dv">1</span>:])</span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Flatten()(inputs)</span>
<span id="cb24-4"><a href="#cb24-4" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Dense(<span class="dv">50</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb24-5"><a href="#cb24-5" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Dense(<span class="dv">50</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb24-6"><a href="#cb24-6" tabindex="-1"></a>    outputs <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>)(x)</span>
<span id="cb24-7"><a href="#cb24-7" tabindex="-1"></a>    <span class="cf">return</span> keras.models.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>outputs,</span>
<span id="cb24-8"><a href="#cb24-8" tabindex="-1"></a>                              name<span class="op">=</span><span class="st">'dense_model'</span>)</span>
<span id="cb24-9"><a href="#cb24-9" tabindex="-1"></a></span>
<span id="cb24-10"><a href="#cb24-10" tabindex="-1"></a>dense_model <span class="op">=</span> create_dense_model()</span>
<span id="cb24-11"><a href="#cb24-11" tabindex="-1"></a>dense_model.summary()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Model: "dense_model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 input_7 (InputLayer)        [(None, 64, 64, 3)]       0

 flatten_5 (Flatten)         (None, 12288)             0

 dense_11 (Dense)            (None, 50)                614450

 dense_12 (Dense)            (None, 50)                2550

 dense_13 (Dense)            (None, 10)                510

=================================================================
Total params: 617510 (2.36 MB)
Trainable params: 617510 (2.36 MB)
Non-trainable params: 0 (0.00 Byte)</code></pre>
</div>
<p>As you can see this model has more parameters than our simple CNN,
let’s train and evaluate it!</p>
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a>compile_model(dense_model)</span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a>history <span class="op">=</span> dense_model.fit(train_images, train_labels, epochs<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a>                    validation_data<span class="op">=</span>(val_images, val_labels))</span>
<span id="cb26-4"><a href="#cb26-4" tabindex="-1"></a>plot_history(history, [<span class="st">'accuracy'</span>, <span class="st">'val_accuracy'</span>])</span></code></pre>
</div>
<figure><img src="fig/04_dense_model_training_history.png" alt="Plot of training accuracy and validation accuracy vs epochs for a model with only dense layers" class="figure mx-auto d-block"></figure><p>As you can see the validation accuracy only reaches about 18%,
whereas the CNN reached about 28% accuracy.</p>
<p>This demonstrates that convolutional layers are a big improvement
over dense layers for these kind of datasets.</p>
</div>
</div>
</div>
</section><section id="refine-the-model"><h2 class="section-heading">9. Refine the model<a class="anchor" aria-label="anchor" href="#refine-the-model"></a>
</h2>
<hr class="half-width"><div id="network-depth" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="network-depth" class="callout-inner">
<h3 class="callout-title">Network depth<a class="anchor" aria-label="anchor" href="#network-depth"></a>
</h3>
<div class="callout-content">
<p>What, do you think, will be the effect of adding a convolutional
layer to your model? Will this model have more or fewer parameters? Try
it out. Create a <code>model</code> that has an additional
<code>Conv2d</code> layer with 50 filters and another MaxPooling2D layer
after the last MaxPooling2D layer. Train it for 10 epochs and plot the
results.</p>
<p><strong>HINT</strong>: The model definition that we used previously
needs to be adjusted as follows:</p>
<div class="codewrapper sourceCode" id="cb27">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a>inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>train_images.shape[<span class="dv">1</span>:])</span>
<span id="cb27-2"><a href="#cb27-2" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(inputs)</span>
<span id="cb27-3"><a href="#cb27-3" tabindex="-1"></a>x <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x)</span>
<span id="cb27-4"><a href="#cb27-4" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb27-5"><a href="#cb27-5" tabindex="-1"></a>x <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x)</span>
<span id="cb27-6"><a href="#cb27-6" tabindex="-1"></a><span class="co"># Add your extra layers here</span></span>
<span id="cb27-7"><a href="#cb27-7" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Flatten()(x)</span>
<span id="cb27-8"><a href="#cb27-8" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dense(<span class="dv">50</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb27-9"><a href="#cb27-9" tabindex="-1"></a>outputs <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>)(x)</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution6" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution6" aria-expanded="false" aria-controls="collapseSolution6">
  <h4 class="accordion-header" id="headingSolution6">Show me the solution</h4>
</button>
<div id="collapseSolution6" class="accordion-collapse collapse" aria-labelledby="headingSolution6" data-bs-parent="#accordionSolution6">
<div class="accordion-body">
<p>We add an extra Conv2D layer after the second pooling layer:</p>
<div class="codewrapper sourceCode" id="cb28">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a><span class="kw">def</span> create_nn_extra_layer():</span>
<span id="cb28-2"><a href="#cb28-2" tabindex="-1"></a>    inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>train_images.shape[<span class="dv">1</span>:])</span>
<span id="cb28-3"><a href="#cb28-3" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(inputs)</span>
<span id="cb28-4"><a href="#cb28-4" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x)</span>
<span id="cb28-5"><a href="#cb28-5" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb28-6"><a href="#cb28-6" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x) <span class="co">#</span></span>
<span id="cb28-7"><a href="#cb28-7" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x) <span class="co"># extra layer</span></span>
<span id="cb28-8"><a href="#cb28-8" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x) <span class="co"># extra layer</span></span>
<span id="cb28-9"><a href="#cb28-9" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Flatten()(x)</span>
<span id="cb28-10"><a href="#cb28-10" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Dense(<span class="dv">50</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb28-11"><a href="#cb28-11" tabindex="-1"></a>    outputs <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>)(x)</span>
<span id="cb28-12"><a href="#cb28-12" tabindex="-1"></a></span>
<span id="cb28-13"><a href="#cb28-13" tabindex="-1"></a>    model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>outputs, name<span class="op">=</span><span class="st">"dollar_street_model"</span>)</span>
<span id="cb28-14"><a href="#cb28-14" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb28-15"><a href="#cb28-15" tabindex="-1"></a></span>
<span id="cb28-16"><a href="#cb28-16" tabindex="-1"></a>model <span class="op">=</span> create_nn_extra_layer()</span></code></pre>
</div>
<p>With the model defined above, we can inspect the number of
parameters:</p>
<div class="codewrapper sourceCode" id="cb29">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a>model.summary()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Model: "dollar_street_model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 input_4 (InputLayer)        [(None, 64, 64, 3)]       0

 conv2d_4 (Conv2D)           (None, 62, 62, 50)        1400

 max_pooling2d_2 (MaxPoolin  (None, 31, 31, 50)        0
 g2D)

 conv2d_5 (Conv2D)           (None, 29, 29, 50)        22550

 max_pooling2d_3 (MaxPoolin  (None, 14, 14, 50)        0
 g2D)

 conv2d_6 (Conv2D)           (None, 12, 12, 50)        22550

 max_pooling2d_4 (MaxPoolin  (None, 6, 6, 50)          0
 g2D)

 flatten_2 (Flatten)         (None, 1800)              0

 dense_4 (Dense)             (None, 50)                90050

 dense_5 (Dense)             (None, 10)                510

=================================================================
Total params: 137060 (535.39 KB)
Trainable params: 137060 (535.39 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</code></pre>
</div>
<p>The number of parameters has decreased by adding this layer. We can
see that the extra layers decrease the resolution from 14x14 to 6x6, as
a result, the input of the Dense layer is smaller than in the previous
network. To train the network and plot the results:</p>
<div class="codewrapper sourceCode" id="cb31">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" tabindex="-1"></a>compile_model(model)</span>
<span id="cb31-2"><a href="#cb31-2" tabindex="-1"></a>history <span class="op">=</span> model.fit(train_images, train_labels, epochs<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb31-3"><a href="#cb31-3" tabindex="-1"></a>                   validation_data<span class="op">=</span>(val_images, val_labels))</span>
<span id="cb31-4"><a href="#cb31-4" tabindex="-1"></a>plot_history(history, [<span class="st">'accuracy'</span>, <span class="st">'val_accuracy'</span>])</span></code></pre>
</div>
<figure><img src="fig/04_training_history_2.png" alt="Plot of training accuracy and validation accuracy vs epochs for the trained model" class="figure mx-auto d-block"></figure></div>
</div>
</div>
</div>
<div id="other-types-of-data" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="other-types-of-data" class="callout-inner">
<h3 class="callout-title">Other types of data<a class="anchor" aria-label="anchor" href="#other-types-of-data"></a>
</h3>
<div class="callout-content">
<p>Convolutional and Pooling layers are also applicable to different
types of data than image data. Whenever the data is ordered in a
(spatial) dimension, and <em>translation invariant</em> features are
expected to be useful, convolutions can be used. Think for example of
time series data from an accelerometer, audio data for speech
recognition, or 3d structures of chemical compounds.</p>
</div>
</div>
</div>
<div id="why-and-when-to-use-convolutional-neural-networks" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="why-and-when-to-use-convolutional-neural-networks" class="callout-inner">
<h3 class="callout-title">Why and when to use convolutional neural
networks<a class="anchor" aria-label="anchor" href="#why-and-when-to-use-convolutional-neural-networks"></a>
</h3>
<div class="callout-content">
<ol style="list-style-type: decimal"><li>Would it make sense to train a convolutional neural network (CNN) on
the penguins dataset and why?</li>
<li>Would it make sense to train a CNN on the weather dataset and
why?</li>
<li>(Optional) Can you think of a different machine learning task that
would benefit from a CNN architecture?</li>
</ol></div>
</div>
</div>
<div id="accordionSolution7" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution7" aria-expanded="false" aria-controls="collapseSolution7">
  <h4 class="accordion-header" id="headingSolution7">Show me the solution</h4>
</button>
<div id="collapseSolution7" class="accordion-collapse collapse" aria-labelledby="headingSolution7" data-bs-parent="#accordionSolution7">
<div class="accordion-body">
<ol style="list-style-type: decimal"><li>No that would not make sense. Convolutions only work when the
features of the data can be ordered in a meaningful way. Pixels for
example are ordered in a spatial dimension. This kind of order cannot be
applied to the features of the penguin dataset. If we would have
pictures or audio recordings of the penguins as input data it would make
sense to use a CNN architecture.</li>
<li>It would make sense, but only if we approach the problem from a
different angle then we did before. Namely, 1D convolutions work quite
well on sequential data such as timeseries. If we have as our input a
matrix of the different weather conditions over time in the past x days,
a CNN would be suited to quickly grasp the temporal relationship over
days.</li>
<li>Some example domains in which CNNs are applied:</li>
</ol><ul><li>Text data</li>
<li>Timeseries, specifically audio</li>
<li>Molecular structures</li>
</ul></div>
</div>
</div>
</div>
</section><section id="dropout"><h2 class="section-heading">Dropout<a class="anchor" aria-label="anchor" href="#dropout"></a>
</h2>
<hr class="half-width"><p>Note that the training loss continues to decrease, while the
validation loss stagnates, and even starts to increase over the course
of the epochs. Similarly, the accuracy for the validation set does not
improve anymore after some epochs. This means we are overfitting on our
training data set.</p>
<p>Techniques to avoid overfitting, or to improve model generalization,
are termed <strong>regularization techniques</strong>. One of the most
versatile regularization technique is <strong>dropout</strong> (<a href="https://jmlr.org/papers/v15/srivastava14a.html" class="external-link">Srivastava et al.,
2014</a>). Dropout means that during each training cycle (one forward
pass of the data through the model) a random fraction of neurons in a
dense layer are turned off. This is described with the dropout rate
between 0 and 1 which determines the fraction of nodes to silence at a
time.</p>
<figure><img src="fig/neural_network_sketch_dropout.png" alt="A sketch of a neural network with and without dropout" class="figure mx-auto d-block"></figure><p>The intuition behind dropout is that it enforces redundancies in the
network by constantly removing different elements of a network. The
model can no longer rely on individual nodes and instead must create
multiple “paths”. In addition, the model has to make predictions with
much fewer nodes and weights (connections between the nodes). As a
result, it becomes much harder for a network to memorize particular
features. At first this might appear a quite drastic approach which
affects the network architecture strongly. In practice, however, dropout
is computationally a very elegant solution which does not affect
training speed. And it frequently works very well.</p>
<p><strong>Important to note:</strong> Dropout layers will only randomly
silence nodes during training! During a predictions step, all nodes
remain active (dropout is off). During training, the sample of nodes
that are silenced are different for each training instance, to give all
nodes a chance to observe enough training data to learn its weights.</p>
<p>Let us add a dropout layer after each pooling layertowards the end of
the network, that randomly drops 80% of the nodes.</p>
<div class="codewrapper sourceCode" id="cb32">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" tabindex="-1"></a><span class="kw">def</span> create_nn_with_dropout():</span>
<span id="cb32-2"><a href="#cb32-2" tabindex="-1"></a>    inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>train_images.shape[<span class="dv">1</span>:])</span>
<span id="cb32-3"><a href="#cb32-3" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(inputs)</span>
<span id="cb32-4"><a href="#cb32-4" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x)</span>
<span id="cb32-5"><a href="#cb32-5" tabindex="-1"></a></span>
<span id="cb32-6"><a href="#cb32-6" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb32-7"><a href="#cb32-7" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x)</span>
<span id="cb32-8"><a href="#cb32-8" tabindex="-1"></a></span>
<span id="cb32-9"><a href="#cb32-9" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb32-10"><a href="#cb32-10" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x)</span>
<span id="cb32-11"><a href="#cb32-11" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Dropout(<span class="fl">0.8</span>)(x) <span class="co"># This is new!</span></span>
<span id="cb32-12"><a href="#cb32-12" tabindex="-1"></a></span>
<span id="cb32-13"><a href="#cb32-13" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Flatten()(x)</span>
<span id="cb32-14"><a href="#cb32-14" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Dense(<span class="dv">50</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb32-15"><a href="#cb32-15" tabindex="-1"></a>    outputs <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>)(x)</span>
<span id="cb32-16"><a href="#cb32-16" tabindex="-1"></a>    model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>outputs, name<span class="op">=</span><span class="st">"dropout_model"</span>)</span>
<span id="cb32-17"><a href="#cb32-17" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb32-18"><a href="#cb32-18" tabindex="-1"></a></span>
<span id="cb32-19"><a href="#cb32-19" tabindex="-1"></a>model_dropout <span class="op">=</span> create_nn_with_dropout()</span>
<span id="cb32-20"><a href="#cb32-20" tabindex="-1"></a>model_dropout.summary()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Model: "dropout_model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 input_5 (InputLayer)        [(None, 64, 64, 3)]       0

 conv2d_7 (Conv2D)           (None, 62, 62, 50)        1400

 max_pooling2d_5 (MaxPoolin  (None, 31, 31, 50)        0
 g2D)

 conv2d_8 (Conv2D)           (None, 29, 29, 50)        22550

 max_pooling2d_6 (MaxPoolin  (None, 14, 14, 50)        0
 g2D)

 conv2d_9 (Conv2D)           (None, 12, 12, 50)        22550

 max_pooling2d_7 (MaxPoolin  (None, 6, 6, 50)          0
 g2D)

 dropout (Dropout)           (None, 6, 6, 50)          0

 flatten_3 (Flatten)         (None, 1800)              0

 dense_6 (Dense)             (None, 50)                90050

 dense_7 (Dense)             (None, 10)                510

=================================================================
Total params: 137060 (535.39 KB)
Trainable params: 137060 (535.39 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</code></pre>
</div>
<p>We can see that the dropout does not alter the dimensions of the
image, and has zero parameters.</p>
<p>We again compile and train the model.</p>
<div class="codewrapper sourceCode" id="cb34">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" tabindex="-1"></a>compile_model(model_dropout)</span>
<span id="cb34-2"><a href="#cb34-2" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" tabindex="-1"></a>history <span class="op">=</span> model_dropout.fit(train_images, train_labels, epochs<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb34-4"><a href="#cb34-4" tabindex="-1"></a>                    validation_data<span class="op">=</span>(val_images, val_labels))</span></code></pre>
</div>
<p>And inspect the training results:</p>
<div class="codewrapper sourceCode" id="cb35">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" tabindex="-1"></a>plot_history(history, [<span class="st">'accuracy'</span>, <span class="st">'val_accuracy'</span>])</span></code></pre>
</div>
<figure><img src="fig/04_training_history_3.png" alt="Plot of training accuracy and validation accuracy vs epochs for the trained model" class="figure mx-auto d-block"></figure><p>Now we see that the gap between the training accuracy and validation
accuracy is much smaller, and that the final accuracy on the validation
set is higher than without dropout.</p>
<div id="vary-dropout-rate" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="vary-dropout-rate" class="callout-inner">
<h3 class="callout-title">Vary dropout rate<a class="anchor" aria-label="anchor" href="#vary-dropout-rate"></a>
</h3>
<div class="callout-content">
<ol style="list-style-type: decimal"><li>What do you think would happen if you lower the dropout rate? Try it
out, and see how it affects the model training.</li>
<li>You are varying the dropout rate and checking its effect on the
model performance, what is the term associated to this procedure?</li>
</ol></div>
</div>
</div>
<div id="accordionSolution8" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution8" aria-expanded="false" aria-controls="collapseSolution8">
  <h4 class="accordion-header" id="headingSolution8">Show me the solution</h4>
</button>
<div id="collapseSolution8" class="accordion-collapse collapse" aria-labelledby="headingSolution8" data-bs-parent="#accordionSolution8">
<div class="accordion-body">
<div class="section level3">
<h3 id="varying-the-dropout-rate">1. Varying the dropout rate<a class="anchor" aria-label="anchor" href="#varying-the-dropout-rate"></a></h3>
<p>The code below instantiates and trains a model with varying dropout
rates. You can see from the resulting plot that the ideal dropout rate
in this case is around 0.9. This is where the val loss is lowest.</p>
<p>Note that it can take a while to train these 6 networks.</p>
<div class="codewrapper sourceCode" id="cb36">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" tabindex="-1"></a><span class="kw">def</span> create_nn_with_dropout(dropout_rate):</span>
<span id="cb36-2"><a href="#cb36-2" tabindex="-1"></a>    inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>train_images.shape[<span class="dv">1</span>:])</span>
<span id="cb36-3"><a href="#cb36-3" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(inputs)</span>
<span id="cb36-4"><a href="#cb36-4" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x)</span>
<span id="cb36-5"><a href="#cb36-5" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Dropout(dropout_rate)(x)</span>
<span id="cb36-6"><a href="#cb36-6" tabindex="-1"></a></span>
<span id="cb36-7"><a href="#cb36-7" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb36-8"><a href="#cb36-8" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x)</span>
<span id="cb36-9"><a href="#cb36-9" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Dropout(dropout_rate)(x)</span>
<span id="cb36-10"><a href="#cb36-10" tabindex="-1"></a></span>
<span id="cb36-11"><a href="#cb36-11" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb36-12"><a href="#cb36-12" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Dropout(dropout_rate)(x)</span>
<span id="cb36-13"><a href="#cb36-13" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Flatten()(x)</span>
<span id="cb36-14"><a href="#cb36-14" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Dense(<span class="dv">50</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb36-15"><a href="#cb36-15" tabindex="-1"></a>    outputs <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>)(x)</span>
<span id="cb36-16"><a href="#cb36-16" tabindex="-1"></a>    model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>outputs, name<span class="op">=</span><span class="st">"dropout_model"</span>)</span>
<span id="cb36-17"><a href="#cb36-17" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb36-18"><a href="#cb36-18" tabindex="-1"></a></span>
<span id="cb36-19"><a href="#cb36-19" tabindex="-1"></a>early_stopper <span class="op">=</span> keras.callbacks.EarlyStopping(monitor<span class="op">=</span><span class="st">'val_loss'</span>, patience<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb36-20"><a href="#cb36-20" tabindex="-1"></a></span>
<span id="cb36-21"><a href="#cb36-21" tabindex="-1"></a>dropout_rates <span class="op">=</span> [<span class="fl">0.2</span>, <span class="fl">0.4</span>, <span class="fl">0.6</span>, <span class="fl">0.8</span>, <span class="fl">0.9</span>, <span class="fl">0.95</span>]</span>
<span id="cb36-22"><a href="#cb36-22" tabindex="-1"></a>val_losses <span class="op">=</span> []</span>
<span id="cb36-23"><a href="#cb36-23" tabindex="-1"></a><span class="cf">for</span> dropout_rate <span class="kw">in</span> dropout_rates:</span>
<span id="cb36-24"><a href="#cb36-24" tabindex="-1"></a>    model_dropout <span class="op">=</span> create_nn_with_dropout(dropout_rate)</span>
<span id="cb36-25"><a href="#cb36-25" tabindex="-1"></a>    compile_model(model_dropout)</span>
<span id="cb36-26"><a href="#cb36-26" tabindex="-1"></a>    model_dropout.fit(train_images, train_labels, epochs<span class="op">=</span><span class="dv">30</span>,</span>
<span id="cb36-27"><a href="#cb36-27" tabindex="-1"></a>                      validation_data<span class="op">=</span>(val_images, val_labels),</span>
<span id="cb36-28"><a href="#cb36-28" tabindex="-1"></a>                      callbacks<span class="op">=</span>[early_stopper]</span>
<span id="cb36-29"><a href="#cb36-29" tabindex="-1"></a>                      )</span>
<span id="cb36-30"><a href="#cb36-30" tabindex="-1"></a></span>
<span id="cb36-31"><a href="#cb36-31" tabindex="-1"></a>    val_loss, val_acc <span class="op">=</span> model_dropout.evaluate(val_images,  val_labels)</span>
<span id="cb36-32"><a href="#cb36-32" tabindex="-1"></a>    val_losses.append(val_loss)</span>
<span id="cb36-33"><a href="#cb36-33" tabindex="-1"></a></span>
<span id="cb36-34"><a href="#cb36-34" tabindex="-1"></a>loss_df <span class="op">=</span> pd.DataFrame({<span class="st">'dropout_rate'</span>: dropout_rates, <span class="st">'val_loss'</span>: val_losses})</span>
<span id="cb36-35"><a href="#cb36-35" tabindex="-1"></a></span>
<span id="cb36-36"><a href="#cb36-36" tabindex="-1"></a></span>
<span id="cb36-37"><a href="#cb36-37" tabindex="-1"></a>sns.lineplot(data<span class="op">=</span>loss_df, x<span class="op">=</span><span class="st">'dropout_rate'</span>, y<span class="op">=</span><span class="st">'val_loss'</span>)</span></code></pre>
</div>
<figure><img src="fig/04_vary_dropout_rate.png" alt="Plot of vall loss vs dropout rate used in the model. The val loss varies between 2.3 and 2.0 and is lowest with a dropout_rate of 0.9" class="figure mx-auto d-block"></figure></div>
<div class="section level3">
<h3 id="term-associated-to-this-procedure">2. Term associated to this procedure<a class="anchor" aria-label="anchor" href="#term-associated-to-this-procedure"></a></h3>
<p>This is called hyperparameter tuning.</p>
</div>
</div>
</div>
</div>
</div>
</section><section id="hyperparameter-tuning"><h2 class="section-heading">Hyperparameter tuning<a class="anchor" aria-label="anchor" href="#hyperparameter-tuning"></a>
</h2>
<hr class="half-width"><p>Recall that hyperparameters are model configuration settings that are
chosen before the training process and affect the model’s learning
behavior and performance, for example the dropout rate. In general, if
you are varying hyperparameters to find the combination of
hyperparameters with the best model performance this is called
hyperparameter tuning. A naive way to do this is to write a for-loop and
train a slightly different model in every cycle. However, it is better
to use the <code>keras_tuner</code> package for this.</p>
<p>Let’s first define a function that creates a neuronal network given 2
hyperparameters, namely the dropout rate and the number of layers:</p>
<div class="codewrapper sourceCode" id="cb37">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" tabindex="-1"></a><span class="kw">def</span> create_nn_with_hp(dropout_rate, n_layers):</span>
<span id="cb37-2"><a href="#cb37-2" tabindex="-1"></a>    inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>train_images.shape[<span class="dv">1</span>:])</span>
<span id="cb37-3"><a href="#cb37-3" tabindex="-1"></a>    x <span class="op">=</span> inputs</span>
<span id="cb37-4"><a href="#cb37-4" tabindex="-1"></a>    <span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(n_layers):</span>
<span id="cb37-5"><a href="#cb37-5" tabindex="-1"></a>        x <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb37-6"><a href="#cb37-6" tabindex="-1"></a>        x <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x)</span>
<span id="cb37-7"><a href="#cb37-7" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Dropout(dropout_rate)(x)</span>
<span id="cb37-8"><a href="#cb37-8" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Flatten()(x)</span>
<span id="cb37-9"><a href="#cb37-9" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Dense(<span class="dv">50</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb37-10"><a href="#cb37-10" tabindex="-1"></a>    outputs <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>)(x)</span>
<span id="cb37-11"><a href="#cb37-11" tabindex="-1"></a>    model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>outputs, name<span class="op">=</span><span class="st">"cifar_model"</span>)</span>
<span id="cb37-12"><a href="#cb37-12" tabindex="-1"></a>    <span class="cf">return</span> model</span></code></pre>
</div>
<p>Now, let’s find the best combination of hyperparameters using grid
search. Grid search is the simplest hyperparameter tuning strategy, you
test all the combinations of predefined values for the hyperparameters
that you want to vary.</p>
<p>For this we will make use of the package <code>keras_tuner</code>, we
can install it by typing in the command line:</p>
<div class="codewrapper sourceCode" id="cb38">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb38-1"><a href="#cb38-1" tabindex="-1"></a><span class="ex">pip</span> install keras_tuner</span></code></pre>
</div>
<p>Note that this can take some time to train (around 5 minutes or
longer).</p>
<div class="codewrapper sourceCode" id="cb39">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" tabindex="-1"></a><span class="im">import</span> keras_tuner</span>
<span id="cb39-2"><a href="#cb39-2" tabindex="-1"></a></span>
<span id="cb39-3"><a href="#cb39-3" tabindex="-1"></a>hp <span class="op">=</span> keras_tuner.HyperParameters()</span>
<span id="cb39-4"><a href="#cb39-4" tabindex="-1"></a></span>
<span id="cb39-5"><a href="#cb39-5" tabindex="-1"></a><span class="kw">def</span> build_model(hp):</span>
<span id="cb39-6"><a href="#cb39-6" tabindex="-1"></a>    <span class="co"># Define values for hyperparameters to try out:</span></span>
<span id="cb39-7"><a href="#cb39-7" tabindex="-1"></a>    n_layers <span class="op">=</span> hp.Int(<span class="st">"n_layers"</span>, min_value<span class="op">=</span><span class="dv">1</span>, max_value<span class="op">=</span><span class="dv">2</span>, step<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb39-8"><a href="#cb39-8" tabindex="-1"></a>    dropout_rate <span class="op">=</span> hp.Float(<span class="st">"dropout_rate"</span>, min_value<span class="op">=</span><span class="fl">0.2</span>, max_value<span class="op">=</span><span class="fl">0.8</span>, step<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb39-9"><a href="#cb39-9" tabindex="-1"></a></span>
<span id="cb39-10"><a href="#cb39-10" tabindex="-1"></a>    model <span class="op">=</span> create_nn_with_hp(dropout_rate, n_layers)</span>
<span id="cb39-11"><a href="#cb39-11" tabindex="-1"></a>    compile_model(model)</span>
<span id="cb39-12"><a href="#cb39-12" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb39-13"><a href="#cb39-13" tabindex="-1"></a></span>
<span id="cb39-14"><a href="#cb39-14" tabindex="-1"></a>tuner <span class="op">=</span> keras_tuner.GridSearch(build_model, objective<span class="op">=</span><span class="st">'val_loss'</span>)</span>
<span id="cb39-15"><a href="#cb39-15" tabindex="-1"></a></span>
<span id="cb39-16"><a href="#cb39-16" tabindex="-1"></a>tuner.search(train_images, train_labels, epochs<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb39-17"><a href="#cb39-17" tabindex="-1"></a>             validation_data<span class="op">=</span>(val_images, val_labels))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Trial 6 Complete [00h 00m 19s]
val_loss: 2.086069345474243

Best val_loss So Far: 2.086069345474243
Total elapsed time: 00h 01m 28s</code></pre>
</div>
<p>Let’s have a look at the results:</p>
<div class="codewrapper sourceCode" id="cb41">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" tabindex="-1"></a>tuner.results_summary()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Results summary
Results in ./untitled_project
Showing 10 best trials
Objective(name="val_loss", direction="min")

Trial 0005 summary
Hyperparameters:
n_layers: 2
dropout_rate: 0.8
Score: 2.086069345474243

Trial 0000 summary
Hyperparameters:
n_layers: 1
dropout_rate: 0.2
Score: 2.101102352142334

Trial 0001 summary
Hyperparameters:
n_layers: 1
dropout_rate: 0.5
Score: 2.1184325218200684

Trial 0003 summary
Hyperparameters:
n_layers: 2
dropout_rate: 0.2
Score: 2.1233835220336914

Trial 0002 summary
Hyperparameters:
n_layers: 1
dropout_rate: 0.8
Score: 2.1370232105255127

Trial 0004 summary
Hyperparameters:
n_layers: 2
dropout_rate: 0.5
Score: 2.143627882003784</code></pre>
</div>
<div id="hyperparameter-tuning-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="hyperparameter-tuning-1" class="callout-inner">
<h3 class="callout-title">Hyperparameter tuning<a class="anchor" aria-label="anchor" href="#hyperparameter-tuning-1"></a>
</h3>
<div class="callout-content">
<p>1: Looking at the grid search results, select all correct
statements:</p>
<ul><li>A. 6 different models were trained in this grid search run, because
there are 6 possible combinations for the defined hyperparameter
values</li>
<li>B. 2 different models were trained, 1 for each hyperparameter that
we want to change</li>
<li>C. 1 model is trained with 6 different hyperparameter
combinations</li>
<li>D. The model with 2 layer and a dropout rate of 0.5 is the best
model with a validation loss of 2.144</li>
<li>E. The model with 2 layers and a dropout rate of 0.8 is the best
model with a validation loss of 2.086</li>
<li>F. We found the model with the best possible combination of dropout
rate and number of layers</li>
</ul><p>2 (Optional): Perform a grid search finding the best combination of
the following hyperparameters: 2 different activation functions: ‘relu’,
and ‘tanh’, and 2 different values for the kernel size: 3 and 4. Which
combination works best?</p>
<p><strong>Hint</strong>: Instead of <code>hp.Int</code> you should now
use <code>hp.Choice("name", ["value1", "value2"])</code> to use
hyperparameters from a predefined set of possible values.</p>
</div>
</div>
</div>
<div id="accordionSolution9" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution9" aria-expanded="false" aria-controls="collapseSolution9">
  <h4 class="accordion-header" id="headingSolution9">Show me the solution</h4>
</button>
<div id="collapseSolution9" class="accordion-collapse collapse" aria-labelledby="headingSolution9" data-bs-parent="#accordionSolution9">
<div class="accordion-body">
<p>1:</p>
<ul><li>A: Correct, 2 values for number of layers (1 and 2) are combined
with 3 values for the dropout rate (0.2, 0.5, 0.8). 2 * 3 = 6
combinations</li>
<li>B: Incorrect, a model is trained for each combination of defined
hyperparameter values</li>
<li>C: Incorrect, it is important to note that you actually train and
test different models for each run of the grid search</li>
<li>D: Incorrect, this is the worst model since the validation loss is
highest</li>
<li>E: Correct, this is the best model with the lowest loss</li>
<li>F: Incorrect, it could be that a different number of layers in
combination with a dropout rate that we did not test (for example 3
layers and a dropout rate of 0.6) could be the best model.</li>
</ul><p>2 (Optional):</p>
<p>You need to adapt the code as follows:</p>
<div class="codewrapper sourceCode" id="cb43">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" tabindex="-1"></a><span class="kw">def</span> create_nn_with_hp(activation_function, kernel_size):</span>
<span id="cb43-2"><a href="#cb43-2" tabindex="-1"></a>    inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>train_images.shape[<span class="dv">1</span>:])</span>
<span id="cb43-3"><a href="#cb43-3" tabindex="-1"></a>    x <span class="op">=</span> inputs</span>
<span id="cb43-4"><a href="#cb43-4" tabindex="-1"></a>    <span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb43-5"><a href="#cb43-5" tabindex="-1"></a>        x <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (kernel_size, kernel_size), activation<span class="op">=</span>activation_function)(x)</span>
<span id="cb43-6"><a href="#cb43-6" tabindex="-1"></a>        x <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x)</span>
<span id="cb43-7"><a href="#cb43-7" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Dropout(<span class="fl">0.2</span>)(x)</span>
<span id="cb43-8"><a href="#cb43-8" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Flatten()(x)</span>
<span id="cb43-9"><a href="#cb43-9" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Dense(<span class="dv">50</span>, activation<span class="op">=</span>activation_function)(x)</span>
<span id="cb43-10"><a href="#cb43-10" tabindex="-1"></a>    outputs <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>)(x)</span>
<span id="cb43-11"><a href="#cb43-11" tabindex="-1"></a>    model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>outputs, name<span class="op">=</span><span class="st">"cifar_model"</span>)</span>
<span id="cb43-12"><a href="#cb43-12" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb43-13"><a href="#cb43-13" tabindex="-1"></a></span>
<span id="cb43-14"><a href="#cb43-14" tabindex="-1"></a>hp <span class="op">=</span> keras_tuner.HyperParameters()</span>
<span id="cb43-15"><a href="#cb43-15" tabindex="-1"></a></span>
<span id="cb43-16"><a href="#cb43-16" tabindex="-1"></a><span class="kw">def</span> build_model(hp):</span>
<span id="cb43-17"><a href="#cb43-17" tabindex="-1"></a>    kernel_size <span class="op">=</span> hp.Int(<span class="st">"kernel_size"</span>, min_value<span class="op">=</span><span class="dv">3</span>, max_value<span class="op">=</span><span class="dv">4</span>, step<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb43-18"><a href="#cb43-18" tabindex="-1"></a>    activation <span class="op">=</span> hp.Choice(<span class="st">"activation"</span>, [<span class="st">"relu"</span>, <span class="st">"tanh"</span>])</span>
<span id="cb43-19"><a href="#cb43-19" tabindex="-1"></a>    model <span class="op">=</span> create_nn_with_hp(activation, kernel_size)</span>
<span id="cb43-20"><a href="#cb43-20" tabindex="-1"></a>    compile_model(model)</span>
<span id="cb43-21"><a href="#cb43-21" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb43-22"><a href="#cb43-22" tabindex="-1"></a></span>
<span id="cb43-23"><a href="#cb43-23" tabindex="-1"></a>tuner <span class="op">=</span> keras_tuner.GridSearch(build_model, objective<span class="op">=</span><span class="st">'val_loss'</span>, project_name<span class="op">=</span><span class="st">'new_project'</span>)</span>
<span id="cb43-24"><a href="#cb43-24" tabindex="-1"></a>tuner.search(train_images, train_labels, epochs<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb43-25"><a href="#cb43-25" tabindex="-1"></a>             validation_data<span class="op">=</span>(val_images, val_labels))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Trial 4 Complete [00h 00m 25s]
val_loss: 2.0591845512390137

Best val_loss So Far: 2.0277602672576904
Total elapsed time: 00h 01m 30s</code></pre>
</div>
<p>Let’s look at the results:</p>
<div class="codewrapper sourceCode" id="cb45">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" tabindex="-1"></a>tuner.results_summary()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Results summary
Results in ./new_project
Showing 10 best trials
Objective(name="val_loss", direction="min")

Trial 0001 summary
Hyperparameters:
kernel_size: 3
activation: tanh
Score: 2.0277602672576904

Trial 0003 summary
Hyperparameters:
kernel_size: 4
activation: tanh
Score: 2.0591845512390137

Trial 0000 summary
Hyperparameters:
kernel_size: 3
activation: relu
Score: 2.123767614364624

Trial 0002 summary
Hyperparameters:
kernel_size: 4
activation: relu
Score: 2.150160551071167</code></pre>
</div>
<p>A kernel size of 3 and <code>tanh</code> as activation function is
the best tested combination.</p>
</div>
</div>
</div>
</div>
<p>Grid search can quickly result in a combinatorial explosion because
all combinations of hyperparameters are trained and tested. Instead,
<code>random search</code> randomly samples combinations of
hyperparemeters, allowing for a much larger look through a large number
of possible hyperparameter combinations.</p>
<p>Next to grid search and random search there are many different
hyperparameter tuning strategies, including <a href="https://en.wikipedia.org/wiki/Neural_architecture_search" class="external-link">neural
architecture search</a> where a separate neural network is trained to
find the best architecture for a model!</p>
</section><section id="share-model"><h2 class="section-heading">10. Share model<a class="anchor" aria-label="anchor" href="#share-model"></a>
</h2>
<hr class="half-width"><p>Let’s save our model</p>
<div class="codewrapper sourceCode" id="cb47">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" tabindex="-1"></a>model.save(<span class="st">'cnn_model'</span>)</span></code></pre>
</div>
</section><section id="conclusion-and-next-steps"><h2 class="section-heading">Conclusion and next steps<a class="anchor" aria-label="anchor" href="#conclusion-and-next-steps"></a>
</h2>
<hr class="half-width"><p>How successful were we with creating a model here? With ten image
classes, and assuming that we would not ask the model to classify an
image that contains none of the given classes of object, a model working
on complete guesswork would be correct 10% of the time. Against this
baseline accuracy of 10%, and considering the diversity and relatively
low resolution of the example images, perhaps our last model’s
validation accuracy of ~30% is not too bad. What could be done to
improve on this performance? We might try adjusting the number of layers
and their parameters, such as the number of units in a layer, or
providing more training data (we were using only a subset of the
original Dollar Street dataset here). Or we could explore some other
deep learning techniques, such as transfer learning, to create more
sophisticated models.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul><li>Convolutional layers make efficient reuse of model parameters.</li>
<li>Pooling layers decrease the resolution of your input</li>
<li>Dropout is a way to prevent overfitting</li>
</ul></div>
</div>
</div>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="3-monitor-the-model.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="5-transfer-learning.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="3-monitor-the-model.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Monitor the training
        </a>
        <a class="chapter-link float-end" href="5-transfer-learning.html" rel="next">
          Next: Transfer learning... 
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>
        
        <a href="https://github.com/carpentries-incubator/deep-learning-intro/edit/main/episodes/4-advanced-layer-types.Rmd" class="external-link">Edit on GitHub</a>
        
	
        | <a href="https://github.com/carpentries-incubator/deep-learning-intro/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/carpentries-incubator/deep-learning-intro/" class="external-link">Source</a></p>
				<p><a href="https://github.com/carpentries-incubator/deep-learning-intro/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:team@carpentries.org">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">
        
        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>
        
        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.4" class="external-link">sandpaper (0.16.4)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.5" class="external-link">pegboard (0.7.5)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.2" class="external-link">varnish (1.0.2)</a></p>
			</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://carpentries-incubator.github.io/deep-learning-intro/4-advanced-layer-types.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "deep learning, keras, lesson, The Carpentries, neural networks",
  "name": "Advanced layer types",
  "creativeWorkStatus": "active",
  "url": "https://carpentries-incubator.github.io/deep-learning-intro/4-advanced-layer-types.html",
  "identifier": "https://carpentries-incubator.github.io/deep-learning-intro/4-advanced-layer-types.html",
  "dateCreated": "2020-10-17",
  "dateModified": "2024-06-11",
  "datePublished": "2024-06-11"
}

  </script><script>
		feather.replace();
	</script><!-- Matomo
    2022-11-07: we have gotten a notification that we have an overage for our
    tracking and I'm pretty sure this has to do with Workbench usage.
    Considering that I am not _currently_ using this tracking because I do not
    yet know how to access the data, I am turning this off for now.
  <script>
    var _paq = window._paq = window._paq || [];
    /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
    _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
    _paq.push(["setDomains", ["*.preview.carpentries.org","*.datacarpentry.github.io","*.datacarpentry.org","*.librarycarpentry.github.io","*.librarycarpentry.org","*.swcarpentry.github.io", "*.carpentries.github.io"]]);
    _paq.push(["setDoNotTrack", true]);
    _paq.push(["disableCookies"]);
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
          var u="https://carpentries.matomo.cloud/";
          _paq.push(['setTrackerUrl', u+'matomo.php']);
          _paq.push(['setSiteId', '1']);
          var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
          g.async=true; g.src='https://cdn.matomo.cloud/carpentries.matomo.cloud/matomo.js'; s.parentNode.insertBefore(g,s);
        })();
  </script>
  End Matomo Code --></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

